{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week-3 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is classification?\n",
    "re-call from week 1: Classifies the entry into small number of different classes/categores, can be numberical or non-numerical.\n",
    "\n",
    "#### Classification can be:\n",
    "* Binary classification: Where the output is 0/1, yes/no, or true/false. negative class would be where the output is 0, and positive class would be where would the output is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why can we not use linear regression with classification?\n",
    "Linear regression tries to best fit all the data, and when data with extreme values are added, can mess the best linear fit and decision boundary, and miss clasify the entries. Alterantively, we can use **logistic regression**!\n",
    "\n",
    "#### What is logistic regression:\n",
    "Logistic regression is an analysis method that uses mathematical functions (most probably sigmoid function) to calculate the probability of an event happening, hence we are using it with binary classification\n",
    "\n",
    "#### How can we interpret the output of logistic regression:\n",
    "In the case that we are using a sigmoid function, the output will always be between 0 and 1 (given the nature of a sigmoid function). Since the output is always between 0 and 1, this output is telling us the probability of the output being classified as positive. e.g. output of 0.7 means that there is a 70% this output being classified as positive, and 30% that this output is a negative.\n",
    "![Sigmoud function](/pictures/sigmoid%20function.png)\n",
    "![Output of Sigmoid function](/pictures/output%20of%20sigmoid%20function.png)\n",
    "\n",
    "\n",
    "#### How will the overall implementation look like?\n",
    "We will still use the linear regression to predict the value of z in the sigmoid function, not to predict the output of the model. As we did before, we will use linear regression to find the best value of z to best fit the sigmoid function to the given training data, and use that model for prediction given a decision boundary (threshold)\n",
    "![logistic regression implementation](/pictures/implementaion%20of%20logisitic%20regression.png)\n",
    "\n",
    "#### What is this decision boundary(threshold)?\n",
    "As the name suggests, it is a threshold value that determines whether the output of this function is going to be positive or negative. e.g. if this threshold value is 0.5, any output equal to or more than this value, will be a positive class, and will be negative class if less.\n",
    "\n",
    "#### How can we use gradient descent to calculate the best value for z:\n",
    "If we tried to use the squared error cost function that we have used with linear regression with logistic regression, it will lead to a non-convex example with multiple local minima, hence gradient descent will not fully converge. However, there is another cost function that we can use to get the best value for z within a sigmoid function that is convex, so that we can use gradient descent to find the least cost with w,b\n",
    "\n",
    "#### What is the cost function to be used with logistic regression:\n",
    "![Logistic regression cost function](/pictures/Logistic%20regression%20cost%20function.png) </br>\n",
    "The loss function above can be rewritten to be easier to implement.\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n",
    "  \n",
    "This is a rather formidable-looking equation. It is less daunting when you consider $y^{(i)}$ can have only two values, 0 and 1. One can then consider the equation in two pieces:  \n",
    "when $ y^{(i)} = 0$, the left-hand term is eliminated:\n",
    "$$\n",
    "\\begin{align}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\\n",
    "&= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "and when $ y^{(i)} = 1$, the right-hand term is eliminated:\n",
    "$$\n",
    "\\begin{align}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\\n",
    "  &=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "OK, with this new logistic loss function, a cost function can be produced that incorporates the loss from all the examples. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we decide on which cost function to use?\n",
    "The above cost function is derived from statistics, and usually this is how the best cost function for choosing parameter values gets chosen, using the maximum likelihood estimation in statistics.\n",
    "\n",
    "#### Using gradient descent with logistic regression:\n",
    "We will still use gradient descent to minimize the cost, using the above cost function, and the derivative of the gradient is similar to that of linear regression. However, The update steps look like the update steps for linear regression, but the definition of f_wb is different. See below: </br>\n",
    "![Gradient descent in logistic regression](/pictures/Gradient%20descent%20with%20logistic%20regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can I make sure that the model will predict non training data well?\n",
    "Your model can be fitting the training data extermely well, but it doesn't predict the non training data well, this is called over-fitting (high variance). Also, your model can be a bad fit for the training example, hence does not do good predicting non training data still, and this is called under-fitting (high bias). You should always go for a model that is just right for the training set and non training set.\n",
    "![overfitting & underfitting](/pictures/overfitting%20and%20underfitting.png)\n",
    "\n",
    "#### How can we address overfitting?\n",
    "There are 3 main ways to address overfitting: </br>\n",
    "1- **Collect more data**: You can try collecting more training data, and that should help reduce the overfitting </br>\n",
    "2- **Select features**: Use your intuition to select less features for your model. More on that in the next courses </br>\n",
    "3- **Regularization**: Regularization is an important technique in which it tries to reduce the weight of the features to near 0 to make some of the features insignificant(still tested with training data), hence less overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does Regularization works?\n",
    "Since Regularization is basically trying to reduce the weights of the features specially those of high degrees, it does this by penalizing those weights during calculating the cost. We will add another term to the cost function, which will penalize the weights and reduce them if possible, by adding a coefficient (lambda) multiplied by the weights, and the model will try to reduce these weights as much as it can to achieve a lower cost. This lambda value is the regularization rate that we will choose when building the model with regularization. Choosing a high regularization rate will cause f_wb to nearly equal to b, as it will reduce all weights and we will only be left with b, since it is not regualarized.</br>\n",
    "This is how the cost function would look:</br>\n",
    "![Regularization term](/pictures/Regularization%20term.png)\n",
    "\n",
    "#### Regularization with linear regression:\n",
    "Since we added a new regularization term to the cost function, the gradient of w will change too, while the gradient of b will not change since this regularization rate is only applied to weights.\n",
    "![Regularization with linear regression](/pictures/Regularization%20with%20linear%20regression.png)\n",
    "\n",
    "#### Regularization with logistic regression:\n",
    "Regularization with logistic regression is similar to linear regression in the sense that we are adding regularization term to the cost function and when calculating the gradient of the whole cost function, this is reflected as follow:\n",
    "![Regularization with logisitic regression](/pictures/Regularization%20with%20logistic%20regression.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
