{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* Multiple features function would look something like this: w1x1 + w2x2 + ... + w3x3 + b\n",
    "* Vectorization has two distinct advantages: makes your code shorter & easier to read, and way faster when computing. np.dot(w,x) + b\n",
    "* **Normal equation** is an algorithm to find w,b in one go without loops, but it only works with linear regression. Also, it becomes quite slow when features > 10,000\n",
    "* Feature scaling is needed for large feature values and large ranges, for gradient descent to run faster, and find the local minima faster.\n",
    "\n",
    "\n",
    "\n",
    "#### How is vectorization faster than normal loop statements?\n",
    "Instead of doing multiple calculations for each data entry with loop statements, vectorization does the magic in the background by multiplying all the entries in background in paralel instead of an entry at once.\n",
    "![How vectorization works in the background](/pictures/Vectorization%20working%20in%20the%20background.png)\n",
    "\n",
    "\n",
    "#### How can we use vectorization with gradient descent?\n",
    "Assuming we have thounds of features, without vectorization we would have to do a loop statement to calculate gradient of weights of these features. On the other hand with vectorization, we can have a vector for weights and a vector for derivatives of these weights, and use the paralel compute hardware.\n",
    "![Using vectorization with gradient descent & multiple features](/pictures/Vectorization%20with%20Gradient%20descent.png)\n",
    "\n",
    "\n",
    "#### Why do we need to scale features to be within close ranges?\n",
    "If one of the features is on a large scale, and another is on a low scale, this will lead to the gradient descent bouncing more to reach the global minima. While if all features are scaled to be close to the same range, this will help the gradient descent to find the local minima with staright lines, instead of bouncing forward and backward, with different scales.\n",
    "\n",
    "\n",
    "#### How can we scale down features?\n",
    "There are multiple ways to scale features (normalize them):\n",
    "1- Feature scaling: Get all feature values to be under 1. You can do this by dividing the feature value by max feature value\n",
    "2- Mean normalization: You can get all feature values between -1 and 1. subtract feature value by mean of all feature values divided by difference between max and min\n",
    "3- Z-score normalization: To get all features around the stanard. subtract the feature value by mean of all feature values and divide by standard deviation of all feature values.\n",
    "\n",
    "#### How can we make sure the gradient descent is converging?\n",
    "The objective of gradient descent is to reduce the cost. We can compare the cost per gradient iteration, which updates the weights and y-intercept.\n",
    "\n",
    "#### How to choose a good learning rate?\n",
    "We can use the cost vs iteration plot to see how our current learning rate looks. If the cost is going up and down, this means that the learning rate is too large. This usually happens when the the gradient skips over the local minima.\n",
    "\n",
    "\n",
    "#### Is feature engineering important?\n",
    "It sometimes can be very crucial. For example, if you have a model that predicts a house price according to its frontage, and depth (length and width), the logical thinking would be to think about the area of the house instead of just length and width. This helps the model predict prices better. Feature engineering is basically you using your intuitions and insights of the model application to combine or transform original given features.\n",
    "\n",
    "\n",
    "#### What is polynomial regression?\n",
    "It will not always be the case for our model to predict the output using a linear function, we can have better fits using more complex functions (suared function, quadratic function, etc...); with polynomial regression we can add features that can help us fit a equation to minimize cost.\n",
    "![Polynomial function that fits the data](/pictures/polynomial%20function.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
