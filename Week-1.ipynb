{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Course by Andrew Ng on Coursera notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Supervised machine learning is giving the machine a data set with features(x) and their output (y) to build a model that can estimate/predict output (Y-hat) given features' values**\n",
    " This supervised machine learning model can be either:\n",
    "* **Regression model**: Predicts/estimates a value out of infinite number of possible values. \\\n",
    "        *e.g. Preidicting houses prices given features* \\\n",
    "        <br>\n",
    "* **Classification model**: Classifies the entry into small number of different classes/categores, can be numberical or non-numerical. \\\n",
    "        *e.g. classifying a picture whether it is a dog or a cat*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression:\n",
    "With linear regression, we are trying to build a model that is trying to recognise a numerical pattern with its given dataset (a set of data with features/inputs and actual outputs). This numerical pattern is a mathematical formula.\n",
    "\n",
    "**How can we know which formula to use?** \\\n",
    "This is where the machine learning comes in! The machine's objective is to learn what is the best mathematical to satisfy the output (y) with the given inputs/featues (x).\n",
    "\n",
    "**What does satisfying the output mean?** \\\n",
    "It means predicting the actual output value in the dataset using its inputs/features, and it does the same for all entries (Xs & Y) in the dataset. The better the formula is at satsfying all data entries from your dataset, the better your model is.\n",
    "\n",
    "In Calculus, a linear function is **f(x) = mx+c** where m is the slope of the function and a is the starting point where x = 0 (y-intercept) and f(x) is the output value y \\\n",
    "To simplify this function and to make it make sense in the course this is how a linear function is represnted: \\two\n",
    "**fw,b(x) = wx+b** where x is the feature's value, w is the slope (weight), b is y-intercept and the prediction/estimate of the formula is y-hat \n",
    "\n",
    "![Linear regression example](/pictures/Linear%20regression%20example.png \"Linear regression graph example\")\n",
    "\n",
    "\n",
    "#### Classification:\n",
    "A Classification model will try to learn the best mathematical formula just like linear regression, but there is one difference. Linear regression will try to find a linear formual to satisfy outputs by predicting output value, but in a Classification model, the formula will by trying to draw a border line instead!\n",
    "This border line will create different zones in the graph, which are classes. Using features, if they fall into a specific zone, it means that they would be classified as the class/zone they fall in.\n",
    "\n",
    "![Classification example](/pictures/Classification%20example.png \"Classification graph example\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function:\n",
    "A cost function is a way of testing your model/mathemtical forumula on how well it satisfy all entries in the dataset. This functionality is needed so your model can compare and chose what is the best values for w and b in the linear formula.\n",
    "There are multiple ways to calculate this cost function, but the course was mainly focused on the **squared loss** function. \\\n",
    "\n",
    "Total loss will be the sum of differences between actual output and calculated value (prediction) for each entry in your dataset given x(feature). It is squared to get the same loss whether it is a negative or positive difference. \\\n",
    "If we go with total loss, the larger the dataset, the larger the cost will become, as you are incrementing the loss for each entry. One way to fix this, is by getting the average sqaured loss; It is also better to divide it by 2m when calculating average loss, where m is the number of entries in the dataset, we will know why later in the course.\n",
    "That leaves us with the below formula for calculating squared loss for a model:\n",
    "            Jw,b(x) = $\\sum_{index=1}^{m} 1/2m (fw,b(x[index]) - y[index])^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the machine compute the best variables for a formula to satisfy the dataset values?** \\\n",
    "*It uses an algorithm called Gradient Descent*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent:\n",
    "Gradient descent is an algorithm that to find the best variables for a mathematical formula given features (x) to try to satisfy/predict an output that is close as possible to the actual output of this feature. \\\n",
    "Gradient descent helps with which way you should move to get less loss that is calculated using squared loss function. \n",
    "\n",
    "Gradient descent relies on the cost function used. If we graph the squared loss function with just changing the slope of the linear equation, the graph will be a parabola with 1 minimum. If we are changing both slope and y-intercept, it will generate a 3D graph with slope as its x-axis, y-intercept as its y-axis, and squared loss value as its z-axis with 1 minimum too since the squared loss formula is a quadratic formula $ax^2 + bx + c$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradient descent](/pictures/Gradient%20Descnet%20Example.png \"Gradient descent example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to calcualte which way to go through Gradient descent to find the best slope and y-intercept values (in case of linear functions)?**\n",
    "\n",
    "Gradient descent uses partial deravitive to calculate the direction of the squared loss function parabola \\\n",
    "![Gradient of loss function](/pictures/Gradient%20of%20loss%20function%20Example.png \"Gradient of loss function\")\n",
    "\n",
    "We then can take steps towards the local minimum. <br> \\\n",
    "**Is knowing which way to move to reduce the loss function output enough?** \\\n",
    "No, We still need to know the step value. How big is our step towards the local. This is called Learning rate (Alpha), which is an important input variable that we adjust when teaching the model. \\\n",
    "\n",
    "**How did we calculate the derivative of the squared loss cost function? for w & b in the linear function?** \\\n",
    "\n",
    "![Deriviative calculation](/pictures/Calculating%20derivative.png) (\"How we got the deriviative of squared loss function for w&b\")\n",
    "\n",
    "\n",
    "**Why cannot we just use large steps value to get to the local minimum? to reduce the iterations and calculations?**\n",
    "If we are not careful with steps size, we can jump over the local minimum.\n",
    "![Learning rate examples](/pictures/Learning%20rate%20examples.png (\"Learning rate examples\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What if we are not using squared loss function to calculate the cost?**\n",
    "We will still build the model the same way. Gather dataset, start with zeros for the inputs(slope and y-intercept in the case of linear functions), calculate loss function, and when using Gradient descent the graph will look different, as you might have multiple local minima, but the gradient of the function should still lead to one of them, which is still good enough. \\\n",
    "![non quadratic cost function](/pictures/non%20quadratic%20loss%20function%20example.jpg (\"Non-quadratic cost function\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
