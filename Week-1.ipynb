{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Course by Andrew Ng on Coursera notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Supervised machine learning is giving the machine a data set with features(x) and their output (y) to build a model that can estimate/predict output (Y-hat) given features' values**\n",
    " This supervised machine learning model can be either:\n",
    "* **Regression model**: Predicts/estimates a value out of infinite number of possible values. \\\n",
    "        *e.g. Preidicting houses prices given features* \\\n",
    "        <br>\n",
    "* **Classification model**: Classifies the entry into small number of different classes/categores, can be numberical or non-numerical. \\\n",
    "        *e.g. classifying a picture whether it is a dog or a cat*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression:\n",
    "With linear regression, we are trying to build a model that is trying to recognise a numerical pattern with its given dataset (a set of data with features/inputs and actual outputs). This numerical pattern is a mathematical formula.\n",
    "\n",
    "**How can we know which formula to use?** \\\n",
    "This is where the machine learning comes in! The machine's objective is to learn what is the best mathematical to satisfy the output (y) with the given inputs/featues (x).\n",
    "\n",
    "**What does satisfying the output mean?** \\\n",
    "It means predicting the actual output value in the dataset using its inputs/features, and it does the same for all entries (Xs & Y) in the dataset. The better the formula is at satsfying all data entries from your dataset, the better your model is.\n",
    "\n",
    "In Calculus, a linear function is **f(x) = mx+c** where m is the slope of the function and a is the starting point where x = 0 (y-intercept) and f(x) is the output value y \\\n",
    "To simplify this function and to make it make sense in the course this is how a linear function is represnted: \\two\n",
    "**fw,b(x) = wx+b** where x is the feature's value, w is the slope (weight), b is y-intercept and the prediction/estimate of the formula is y-hat \n",
    "\n",
    "![Linear regression example](/pictures/Linear%20regression%20example.png \"Linear regression graph example\")\n",
    "\n",
    "\n",
    "#### Classification:\n",
    "A Classification model will try to learn the best mathematical formula just like linear regression, but there is one difference. Linear regression will try to find a linear formual to satisfy outputs by predicting output value, but in a Classification model, the formula will by trying to draw a border line instead!\n",
    "This border line will create different zones in the graph, which are classes. Using features, if they fall into a specific zone, it means that they would be classified as the class/zone they fall in.\n",
    "\n",
    "![Classification example](/pictures/Classification%20example.png \"Classification graph example\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function:\n",
    "A cost function is a way of testing your model/mathemtical forumula on how well it satisfy all entries in the dataset. This functionality is needed so your model can compare and chose what is the best values for w and b in the linear formula.\n",
    "There are multiple ways to calculate this cost function, but the course was mainly focused on the **squared loss** function. \\\n",
    "\n",
    "Total loss will be the sum of differences between actual output and calculated value (prediction) for each entry in your dataset given x(feature). It is squared to get the same loss whether it is a negative or positive difference. \\\n",
    "If we go with total loss, the larger the dataset, the larger the cost will become, as you are incrementing the loss for each entry. One way to fix this, is by getting the average sqaured loss; It is also better to divide it by 2m when calculating average loss, where m is the number of entries in the dataset, we will know why later in the course.\n",
    "That leaves us with the below formula for calculating squared loss for a model:\n",
    "            Jw,b(x) = $\\sum_{index=1}^{m} 1/2m (fw,b(x[index]) - y[index])^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the machine compute the best variables for a formula to satisfy the dataset values?** \\\n",
    "*It uses an algorithm called Gradient Descent*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent:\n",
    "Gradient descent is an algorithm that to find the best variables for a mathematical formula given features (x) to try to satisfy/predict an output that is close as possible to the actual output of this feature. \\\n",
    "\n",
    "Gradient descent relies on the cost function used. If we graph the squared loss function with just changing the slope of the linear equation, the graph will be a parabola with 1 minimum. If we are changing both slope and y-intercept, it will generate a 3D graph with slope as its x-axis, y-intercept as its y-axis, and squared loss value as its z-axis with 1 minimum too since the squared loss formula is a quadratic formula $ax^2 + bx + c$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
